{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amir-asari/Introduction_to_Huggingface/blob/main/2_UsingTransformers/4_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAlOIF5FH1pH"
      },
      "source": [
        "Stages of learning a new Language\n",
        "1. **Learn vocabulary**\n",
        "2. Learn meaning of words\n",
        "3. Learn syntax & grammar rules\n",
        "4. Speak the language\n",
        "\n",
        "Similarly, In Artificial NLP\n",
        "1. **Build vocabulary with `tokenizer`**\n",
        "2. Learn semantic meaning via embedding_value, in model from data\n",
        "3. Learn syntax & grammar rules, in model from data\n",
        "4. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4AYLelhH1pJ"
      },
      "source": [
        "## 1. learn vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwftPlthH1pK"
      },
      "source": [
        "Manual Methods\n",
        "1. `tokenizer.tokenize()`\n",
        "2. `tokenizer.convert_tokens_to_ids()`\n",
        "3. `tokenizer.decode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1O0rsNeH1pK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2SL_Q25H1pL",
        "outputId": "6aaf6e53-c411-4acc-d576-423f1bcfac74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence into words \t = ['good', 'morning'], \n",
            " numerical mapping \t= [1363, 2106], \n",
            " numbers_to_string \t= good morning\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\", model_kwargs= {\"force_download\": True})\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\", model_kwargs= {\"force_download\": True})\n",
        "\n",
        "# tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
        "sentences_to_tokenize = [\n",
        "                            \"good morning\",\n",
        "                            \"hey\",\n",
        "                            \"bye bye\"\n",
        "\n",
        "                    ]\n",
        "\n",
        "tokens          = tokenizer.tokenize(\"good morning\")\n",
        "numeric_ids     = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "decoded_string  = tokenizer.decode(numeric_ids)\n",
        "print(f'sentence into words \\t = {tokens}, \\n numerical mapping \\t= {numeric_ids}, \\n numbers_to_string \\t= {decoded_string}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQZrqoSQH1pM",
        "outputId": "acbdd7fa-2cfd-4024-9faa-8bdd1e7ecdbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method PreTrainedTokenizer.convert_tokens_to_ids of BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_from_huggingface = tokenizer\n",
        "tokenizer_from_huggingface.decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYB9xhF3H1pN"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWeabLb7H1pN",
        "outputId": "1dc18971-23c4-4919-fefc-412c440624c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'good good'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "'good morning'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "'morning good'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "'morning morning'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokenizer.decode([1363, 1363])\n",
        "tokenizer.decode([1363, 2106])\n",
        "tokenizer.decode([2106, 1363])\n",
        "tokenizer.decode([2106, 2106])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt6dGVTAH1pO",
        "outputId": "3a3f044a-0d7b-48bc-d619-611a9b0e2e3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1363, 2106, 2490]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_tokens_to_ids (tokenizer.tokenize(\"good morning everyone\") )\n",
        "token_ids = tokenizer(\"good morning everyone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtIP3TnJH1pP",
        "outputId": "b7366bd0-31c4-43f3-feba-687ad00418b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids \t [101, 1363, 2106, 2490, 102]\n",
            "token_type_ids \t [0, 0, 0, 0, 0]\n",
            "attention_mask \t [1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "for key in token_ids.keys():\n",
        "    print(f'{key} \\t {token_ids[key]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsavdBSaH1pP"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer( sentences_to_tokenize , ) # padding = True, return_tensors = \"pt\")\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VqHQWRwH1pP"
      },
      "source": [
        "## 2. meaning & embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO29g0rrH1pP"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "config = transformers.BertConfig()\n",
        "model  = transformers.BertModel(config).from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkfc8VqbH1pQ",
        "outputId": "95fd8017-9c7b-411a-f8e3-15d46649831c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertEmbeddings(\n",
              "  (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "  (position_embeddings): Embedding(512, 768)\n",
              "  (token_type_embeddings): Embedding(2, 768)\n",
              "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMPv1sF5H1pQ",
        "outputId": "bd9f18ec-8c84-4cad-dc04-5fa1e79c4107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "word_to_find_meaning_of = \"love\"\n",
        "\n",
        "output = model( ** tokenizer(word_to_find_meaning_of, return_tensors=\"pt\"))\n",
        "\n",
        "print(output['last_hidden_state'][0][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5iG2p_oH1pQ"
      },
      "outputs": [],
      "source": [
        "multiple_sentences = [\"run chocolates are running out\"      # 5 Words English Sentece\n",
        "                    , \"run there is fire in fireplace\"      # 5 Words English Sentece\n",
        "                    , \"run fire is spreading everywhere\" ]    # 5 Words English Sentece"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}